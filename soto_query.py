from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage, Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# 1Ô∏è‚É£ Configuraci√≥n para trabajar 100% local
Settings.embed_model = HuggingFaceEmbedding(model_name="sentence-transformers/all-MiniLM-L6-v2")
Settings.llm = None  # üîπ Desactivamos cualquier modelo que pida API externa

# 2Ô∏è‚É£ Cargar √≠ndice existente
storage_context = StorageContext.from_defaults(persist_dir="./soto_index")
index = load_index_from_storage(storage_context)

# 3Ô∏è‚É£ Motor de consultas
query_engine = index.as_query_engine()

# 4Ô∏è‚É£ Bucle de preguntas
while True:
    pregunta = input("Pregunta para SOTO (o escribe 'salir' para terminar): ")
    if pregunta.lower() == "salir":
        break
    
    respuesta = query_engine.query(pregunta)
    print(f"\nüí¨ SOTO: {respuesta}\n")
